{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "usage: ipykernel_launcher.py [-h] [-l LENGTH] filepath\n",
      "ipykernel_launcher.py: error: unrecognized arguments: -f\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "2",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[0;31mSystemExit\u001b[0m\u001b[0;31m:\u001b[0m 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/IPython/core/interactiveshell.py:2971: UserWarning: To exit: use 'exit', 'quit', or Ctrl-D.\n",
      "  warn(\"To exit: use 'exit', 'quit', or Ctrl-D.\", stacklevel=1)\n"
     ]
    }
   ],
   "source": [
    "import argparse\n",
    "\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from string import punctuation\n",
    "from nltk.probability import FreqDist\n",
    "from heapq import nlargest\n",
    "from collections import defaultdict\n",
    "\n",
    "def main():\n",
    "    \"\"\" Drive the process from argument to output \"\"\" \n",
    "    args = parse_arguments()\n",
    "\n",
    "    content = read_file(args.filepath)\n",
    "    content = sanitize_input(content)\n",
    "\n",
    "    sentence_tokens, word_tokens = tokenize_content(content)  \n",
    "    sentence_ranks = score_tokens(word_tokens, sentence_tokens)\n",
    "\n",
    "    return summarize(sentence_ranks, sentence_tokens, args.length)\n",
    "\n",
    "def parse_arguments():\n",
    "    \"\"\" Parse command line arguments \"\"\" \n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument('filepath', help='File name of text to summarize')\n",
    "    parser.add_argument('-l', '--length', default=4, help='Number of sentences to return')\n",
    "    args = parser.parse_args()\n",
    "\n",
    "    return args\n",
    "\n",
    "def read_file(path):\n",
    "    \"\"\" Read the file at designated path and throw exception if unable to do so \"\"\" \n",
    "    try:\n",
    "        with open(path, 'r') as file:\n",
    "            return file.read()\n",
    "\n",
    "    except IOError as e:\n",
    "        print(\"Fatal Error: File ({}) could not be locaeted or is not readable.\".format(path))\n",
    "\n",
    "def sanitize_input(data):\n",
    "    \"\"\" \n",
    "    Currently just a whitespace remover. More thought will have to be given with how \n",
    "    to handle sanitzation and encoding in a way that most text files can be successfully\n",
    "    parsed\n",
    "    \"\"\"\n",
    "    replace = {\n",
    "        ord('\\f') : ' ',\n",
    "        ord('\\t') : ' ',\n",
    "        ord('\\n') : ' ',\n",
    "        ord('\\r') : None\n",
    "    }\n",
    "\n",
    "    return data.translate(replace)\n",
    "\n",
    "def tokenize_content(content):\n",
    "    \"\"\"\n",
    "    Accept the content and produce a list of tokenized sentences, \n",
    "    a list of tokenized words, and then a list of the tokenized words\n",
    "    with stop words built from NLTK corpus and Python string class filtred out. \n",
    "    \"\"\"\n",
    "    stop_words = set(stopwords.words('english') + list(punctuation))\n",
    "    words = word_tokenize(content.lower())\n",
    "    \n",
    "    return [\n",
    "        sent_tokenize(content),\n",
    "        [word for word in words if word not in stop_words]    \n",
    "    ]\n",
    "\n",
    "def score_tokens(filterd_words, sentence_tokens):\n",
    "    \"\"\"\n",
    "    Builds a frequency map based on the filtered list of words and \n",
    "    uses this to produce a map of each sentence and its total score\n",
    "    \"\"\"\n",
    "    word_freq = FreqDist(filterd_words)\n",
    "\n",
    "    ranking = defaultdict(int)\n",
    "\n",
    "    for i, sentence in enumerate(sentence_tokens):\n",
    "        for word in word_tokenize(sentence.lower()):\n",
    "            if word in word_freq:\n",
    "                ranking[i] += word_freq[word]\n",
    "\n",
    "    return ranking\n",
    "\n",
    "def summarize(ranks, sentences, length):\n",
    "    \"\"\"\n",
    "    Utilizes a ranking map produced by score_token to extract\n",
    "    the highest ranking sentences in order after converting from\n",
    "    array to string.  \n",
    "    \"\"\"\n",
    "    if int(length) > len(sentences): \n",
    "        print(\"Error, more sentences requested than available. Use --l (--length) flag to adjust.\")\n",
    "        exit()\n",
    "\n",
    "    indexes = nlargest(length, ranks, key=ranks.get)\n",
    "    final_sentences = [sentences[j] for j in sorted(indexes)]\n",
    "    return ' '.join(final_sentences) \n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    print(main())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sumy_scope(filename):\n",
    "    from sumy.parsers.plaintext import PlaintextParser\n",
    "    from sumy.nlp.tokenizers import Tokenizer\n",
    "    file = filename\n",
    "    parser = PlaintextParser.from_file(file, Tokenizer(\"english\"))\n",
    "\n",
    "    from sumy.summarizers.luhn import LuhnSummarizer\n",
    "    summarizer = LuhnSummarizer()\n",
    "    summary =summarizer(parser.document,2)\n",
    "    for sentence in summary:\n",
    "        print(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "And the same processes at work on Greenland's glaciers at the top of the world could send vast sections of Antarctica's ice sheet into the sea as well, raising ocean levels even further.\n",
      "Departing from Kangerlussuaq, VOA visited East GRIP and other remote corners of Greenland with the 109th Airlift Wing of the U.S. Air National Guard for a firsthand look at science in action at the leading edge of climate change.\n"
     ]
    }
   ],
   "source": [
    "sumy_scope(\"Greenland-Melting-Full.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sanitize_input(data):\n",
    "    \"\"\" \n",
    "    Currently just a whitespace remover. More thought will have to be given with how \n",
    "    to handle sanitzation and encoding in a way that most text files can be successfully\n",
    "    parsed\n",
    "    \"\"\"\n",
    "    replace = {\n",
    "        ord('\\f') : ' ',\n",
    "        ord('\\t') : ' ',\n",
    "        ord('\\n') : ' ',\n",
    "        ord('\\r') : None\n",
    "    }\n",
    "\n",
    "    return data.translate(replace)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Greenland-Melting-Full.txt'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sanitize_input(\"Greenland-Melting-Full.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
